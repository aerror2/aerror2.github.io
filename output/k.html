
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>如何在ms-swift 微调训练deepseekvl2时使用sageattention</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-okaidia.min.css">

    <style>
        /* 保留CSDN常用的文本颜色类 */
        .text-red, [class*="text-red"] { color: red !important; }
        .text-blue, [class*="text-blue"] { color: blue !important; }
        .text-green, [class*="text-green"] { color: green !important; }
        .text-yellow, [class*="text-yellow"] { color: #FFD700 !important; }
        .text-purple, [class*="text-purple"] { color: purple !important; }
        .text-orange, [class*="text-orange"] { color: orange !important; }
        
        /* 保留常见的CSDN文本样式类 */
        .bold, .strong, [class*="bold"], [class*="strong"] { font-weight: bold !important; }
        .italic, .em, [class*="italic"], [class*="em"] { font-style: italic !important; }
        .underline, [class*="underline"] { text-decoration: underline !important; }
        
        /* 确保内联样式不被覆盖 */
        .article-content * { color: initial; background-color: initial; font-weight: initial; font-style: initial; text-decoration: initial; }
        
        /* 保留font标签的color属性 */
        .article-content font[color="red"] { color: red !important; }
        .article-content font[color="blue"] { color: blue !important; }
        .article-content font[color="green"] { color: green !important; }
        .article-content font[color="yellow"] { color: yellow !important; }
        .article-content font[color="purple"] { color: purple !important; }
        .article-content font[color="orange"] { color: orange !important; }
        .article-content font[color="black"] { color: black !important; }
        .article-content font[color="white"] { color: white !important; }
        .article-content font[color="gray"] { color: gray !important; }
        .article-content font[color="#000000"] { color: #000000 !important; }
        .article-content font[color="#FF0000"] { color: #FF0000 !important; }
        .article-content font[color="#00FF00"] { color: #00FF00 !important; }
        .article-content font[color="#0000FF"] { color: #0000FF !important; }
        .article-content font[color="#FFFF00"] { color: #FFFF00 !important; }
        .article-content font[color="#00FFFF"] { color: #00FFFF !important; }
        .article-content font[color="#FF00FF"] { color: #FF00FF !important; }
    </style>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 1200px; margin: 0 auto; background-color: #f8f9fa; }
        img { max-width: 100%; height: auto; }
        pre { background-color: #2d2d2d; padding: 10px; border-radius: 5px; overflow-x: auto; margin: 1em 0; }
        code { font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace; }
        pre[class*="language-"] { position: relative; }
        pre[class*="language-"] .copy-to-clipboard-button { position: absolute; top: 5px; right: 5px; background: rgba(0,0,0,0.3); color: white; border: none; border-radius: 3px; padding: 5px 10px; font-size: 0.8em; cursor: pointer; }
        pre[class*="language-"] .copy-to-clipboard-button:hover { background: rgba(0,0,0,0.5); }
        pre[class*="language-"] code { user-select: all; -webkit-user-select: all; -moz-user-select: all; -ms-user-select: all; }
        pre[class*="language-"]:hover .copy-to-clipboard-button { display: block; }
        pre[class*="language-"] .copy-to-clipboard-button { display: none; }
        .source-link { margin-bottom: 15px; color: #666; }
        h1 { font-size: 28px; margin-bottom: 15px; color: #333; }
        .article-content { margin-top: 25px; background-color: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        
        /* 文章元数据样式 */
        .article-meta { 
            display: flex; 
            flex-wrap: wrap; 
            gap: 15px; 
            font-size: 14px; 
            color: #666; 
            margin-bottom: 15px; 
        }
        .meta-item { 
            display: flex; 
            align-items: center; 
        }
        .meta-item i { 
            margin-right: 5px; 
            color: #0066cc; 
        }
        
        /* 标签样式 */
        .article-tags { 
            margin-bottom: 15px; 
        }
        .tag { 
            display: inline-block; 
            background-color: #e6f2ff; 
            color: #0066cc; 
            padding: 3px 8px; 
            border-radius: 4px; 
            font-size: 12px; 
            margin-right: 5px; 
        }
        
        /* 描述样式 */
        .article-description { 
            background-color: #f5f5f5; 
            padding: 15px; 
            border-left: 4px solid #0066cc; 
            margin-bottom: 20px; 
            font-style: italic; 
            color: #555; 
        }
        .token.comment, .token.prolog, .token.doctype, .token.cdata { color: #8292a2; }
        .token.punctuation { color: #f8f8f2; }
        .token.namespace { opacity: .7; }
        .token.property, .token.tag, .token.constant, .token.symbol, .token.deleted { color: #f92672; }
        .token.boolean, .token.number { color: #ae81ff; }
        .token.selector, .token.attr-name, .token.string, .token.char, .token.builtin, .token.inserted { color: #a6e22e; }
        .token.operator, .token.entity, .token.url, .language-css .token.string, .style .token.string, .token.variable { color: #f8f8f2; }
        .token.atrule, .token.attr-value, .token.function, .token.class-name { color: #e6db74; }
        .token.keyword { color: #66d9ef; }
        .token.regex, .token.important { color: #fd971f; }
        .token.important, .token.bold { font-weight: bold; }
        .token.italic { font-style: italic; }
        .token.entity { cursor: help; }
        
        /* 添加header和footer样式 */
        .header { 
            padding: 10px 0; 
            margin-bottom: 20px; 
            border-bottom: 1px solid #eee; 
        }
        .header a { 
            text-decoration: none; 
            color: #007bff; 
            font-weight: bold; 
        }
        .header a:hover { 
            text-decoration: underline; 
        }
        .footer { 
            margin-top: 30px; 
            padding-top: 20px; 
            border-top: 1px solid #eee; 
            color: #666; 
            font-size: 0.9em; 
            text-align: center; 
        }
    </style>
</head>
<body>
    <div class="header">
        <a href="index.html">← 返回目录</a>
    </div>
    
    <h1>如何在ms-swift 微调训练deepseekvl2时使用sageattention</h1>
    <div class="source-link">原文链接: <a href="https://blog.csdn.net/aerror/article/details/146589557" target="_blank">https://blog.csdn.net/aerror/article/details/146589557</a></div>
    
    <div class="article-meta">
        <div class="meta-item"><i class="far fa-calendar-alt"></i> 发布时间: 2025-03-28 11:22:07</div>
        <div class="meta-item"><i class="far fa-eye"></i> 浏览量: 510</div>
        
        <div class="meta-item"><i class="far fa-star"></i> 收藏数: 9</div>
        <div class="meta-item"><i class="far fa-thumbs-up"></i> 点赞数: 4</div>
    </div>
    
    <div class="article-tags">标签: <span class="tag">深度学习</span>, <span class="tag">机器学习</span>, <span class="tag">人工智能</span></div>
    
    <div class="article-description">1.本质上sageattention是sdpa，SDPA的全称为Scaled Dot-Product Attention, 属于乘性注意力机制， 简单一句话来说就是，根据Query (Q)与Key之间的匹配度来对Value进行加权，而事实上不管是Query, Key还是Value都来自于输入，因此所谓的SDPA本质上是对输入信息信息进行重组。因此，deepseekvl2无法直接简单使用sageattion,我们需要改一下deepseek的开源代码，才有可能用上sageattion.</div>
    
    <div class="article-content">
        <div class="htmledit_views atom-one-dark" id="content_views">
<p>        sageattention 据说比flash_atten_2还要快很多。 但是如何在deepseekvl2这训练这里把它用上呢？</p>
<p>        1.本质上sageattention是sdpa，SDPA的全称为Scaled Dot-Product Attention, 属于乘性注意力机制， 简单一句话来说就是，根据Query (Q)与Key之间的匹配度来对Value进行加权，而事实上不管是Query, Key还是Value都来自于输入，因此所谓的SDPA本质上是对输入信息信息进行重组。</p>
<p>       2. sageattention使用了Triton这个包，它可以把python的代码编译成目标机器码，大大加速这个运算的速度。</p>
<p>        3. 从官方例子可以看到，它本质的工作原理就是简单的替换torch.nn.functional.scaled_dot_product_attention这个函数，示例如下：</p>
<pre><code class="language-python hljs">from sageattention import sageattn
import torch.nn.functional as F
。。。
    F.scaled_dot_product_attention = sageattn
</code></pre>
<p><span style="color:#fe2c24">        所以，要用上sageatten，其实只需要原来的模型支持sdpa的注意力机制即可。</span></p>
<p></p>
<p>        但很不幸，从deepseekvl2官方开源github可以看到DeepseekVLV2ForCausalLM和DeepseekV2ForCausalLM都是不支持sdpa的，这个两个类都没有声明_supports_sdpa = True</p>
<pre><code class="language-python hljs">ATTENTION_CLASSES = {
    "eager": DeepseekV2Attention,
    "flash_attention_2": DeepseekV2FlashAttention2,

    "mla_eager": DeepseekV2Attention,
    "mla_flash_attention_2": DeepseekV2FlashAttention2,

    "mha_eager": LlamaAttention,
    "mha_flash_attention_2": LlamaFlashAttention2
}
</code></pre>
<p>        这个attention_class也没有sdpa的实现。</p>
<p><span style="color:#fe2c24">        因此，deepseekvl2无法直接简单使用sageattion,我们需要改一下deepseek的开源代码，才有可能用上sageattion.</span></p>
<p></p>
<p>        修改步骤如下：</p>
<p>1. 首先要让DeepseekVLV2ForCausalLM和DeepseekV2ForCausalLM先支持sdpa,添加_supports_sdpa,这样transformers/modeling_utils.py的_check_and_enable_sdpa才可以检查通过.</p>
<pre><code class="language-python hljs">class DeepseekV2ForCausalLM(DeepseekV2PreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    _supports_sdpa = True</code></pre>
<pre><code class="language-python hljs">class DeepseekVLV2ForCausalLM(DeepseekVLV2PreTrainedModel):
    _supports_sdpa = True</code></pre>
<p>2.然后在DeepSeek-VL2/deepseek_vl2/models/modeling_deepseek.py添加sdpa的attention的实现类，我们让它继承LlamaAttention，如下，其实这个直接抄的LlamaSdpaAttention，copy是为了修改方便，实现如下：</p>
<pre><code class="language-python hljs">
class DeepSeekSdpaAttention(LlamaAttention):
    """
    Deepseek attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    `DeepseekV2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    """

    # Adapted from LlamaAttention.forward
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46
        **kwargs,
    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if output_attentions:
            # TODO: Improve this warning with e.g. `model.config.attn_implementation = "manual"` once this is implemented.
            logger.warning_once(
                "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, "
                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
            )
            return super().forward(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
            )

        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used
        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        if position_embeddings is None:
            logger.warning_once(
                "The attention layers in this model are transitioning from computing the RoPE embeddings internally "
                "through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed "
                "`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be "
                "removed and `position_embeddings` will be mandatory."
            )
            cos, sin = self.rotary_emb(value_states, position_ids)
        else:
            if isinstance(position_embeddings, torch.Tensor):
                cos, sin = self.rotary_emb(value_states, position_ids)
            else:
                cos, sin = position_embeddings


        query_states, key_states = apply_rotary_pos_emb2(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        causal_mask = attention_mask
        if attention_mask is not None:
            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]

        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,
        # Reference: https://github.com/pytorch/pytorch/issues/112577.
        if query_states.device.type == "cuda" and causal_mask is not None:
            query_states = query_states.contiguous()
            key_states = key_states.contiguous()
            value_states = value_states.contiguous()

        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment
        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.
        is_causal = True  #if causal_mask is None and q_len &gt; 1 else False

        attn_output = sageattn(
            query_states,
            key_states,
            value_states,
            attn_mask=causal_mask,
            dropout_p=self.attention_dropout if self.training else 0.0,
            is_causal=is_causal,
        )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(bsz, q_len, -1)

        attn_output = self.o_proj(attn_output)

        return attn_output, None, past_key_value
</code></pre>
<p>3. 修改modeling_deepseek.py的ATTENTION_CLASSES，加上sdpa支持，如下：</p>
<pre><code class="language-python hljs">
ATTENTION_CLASSES = {
    "eager": DeepseekV2Attention,
    "flash_attention_2": DeepseekV2FlashAttention2,

    "mla_eager": DeepseekV2Attention,
    "mla_flash_attention_2": DeepseekV2FlashAttention2,

    "mha_eager": LlamaAttention,
    "mha_flash_attention_2": LlamaFlashAttention2,
    "mha_sdpa": DeepSeekSdpaAttention
}
</code></pre>
<p>4.使用的sageattion和Triton的版本如下：</p>
<pre><code class="language-python hljs">Name: sageattention
Version: 1.0.6

Name: triton
Version: 3.2.0</code></pre>
<p>5. 训练测试，</p>
<pre><code class="language-python hljs">swift sft  --model "deepseek-ai/deepseek-vl2-tiny"  --dataset  ../TEST.json  --attn_impl sdp

。。。
using attn_implementation: mha_sdpa
[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}

。。。
[INFO:swift] End time of running main: 2025-03-28 10:43:29.304164
</code></pre>
<p>成功进行了训练。</p>
<p></p>
</div>
    </div>
    
    <div class="footer">
        <p>发表时间: 2025-03-28 11:22:07</p>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        // 处理代码块，添加语言类和复制功能
        document.addEventListener('DOMContentLoaded', function() {            
            // 为没有指定语言的代码块添加默认语言
            document.querySelectorAll('pre code:not([class*="language-"])').forEach(function(block) {
                block.className = 'hljs language-javascript';
            });
            
            // 确保所有代码块都有hljs类
            document.querySelectorAll('pre code[class*="language-"]:not([class*="hljs"])').forEach(function(block) {
                block.className = 'hljs ' + block.className;
            });
            
            // 为所有代码块添加自定义复制按钮
            document.querySelectorAll('pre').forEach(function(pre) {
                // 创建复制按钮
                var copyButton = document.createElement('button');
                copyButton.className = 'copy-button';
                copyButton.textContent = '复制';
                copyButton.style.position = 'absolute';
                copyButton.style.top = '5px';
                copyButton.style.right = '5px';
                copyButton.style.background = 'rgba(0,0,0,0.3)';
                copyButton.style.color = 'white';
                copyButton.style.border = 'none';
                copyButton.style.borderRadius = '3px';
                copyButton.style.padding = '5px 10px';
                copyButton.style.fontSize = '0.8em';
                copyButton.style.cursor = 'pointer';
                copyButton.style.display = 'none';
                
                // 鼠标悬停时显示按钮
                pre.addEventListener('mouseenter', function() {
                    copyButton.style.display = 'block';
                });
                
                pre.addEventListener('mouseleave', function() {
                    copyButton.style.display = 'none';
                });
                
                // 点击复制按钮时复制代码
                copyButton.addEventListener('click', function() {
                    var code = pre.querySelector('code');
                    var text = code.textContent || code.innerText;
                    
                    // 使用现代的 Clipboard API
                    if (navigator.clipboard && navigator.clipboard.writeText) {
                        navigator.clipboard.writeText(text)
                            .then(function() {
                                copyButton.textContent = '已复制!';
                                setTimeout(function() {
                                    copyButton.textContent = '复制';
                                }, 2000);
                            })
                            .catch(function(err) {
                                console.error('Clipboard API 复制失败:', err);
                                // 如果 Clipboard API 失败，尝试使用传统方法
                                fallbackCopyToClipboard();
                            });
                    } else {
                        // 对于不支持 Clipboard API 的浏览器，使用传统方法
                        fallbackCopyToClipboard();
                    }
                    
                    // 传统复制方法作为后备
                    function fallbackCopyToClipboard() {
                        // 创建临时文本区域
                        var textArea = document.createElement('textarea');
                        textArea.value = text;
                        textArea.style.position = 'fixed';
                        textArea.style.left = '-9999px';
                        textArea.style.top = '0';
                        document.body.appendChild(textArea);
                        textArea.focus();
                        textArea.select();
                        
                        try {
                            var successful = document.execCommand('copy');
                            if (successful) {
                                copyButton.textContent = '已复制!';
                                setTimeout(function() {
                                    copyButton.textContent = '复制';
                                }, 2000);
                            } else {
                                copyButton.textContent = '复制失败';
                            }
                        } catch (err) {
                            copyButton.textContent = '复制失败';
                            console.error('传统复制方法失败:', err);
                        }
                        
                        document.body.removeChild(textArea);
                    }
                });
                
                // 添加按钮到代码块
                pre.style.position = 'relative';
                pre.appendChild(copyButton);
            });
            
            // 确保Prism重新高亮所有代码块
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>
